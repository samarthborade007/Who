{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Audio\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "root = \"../data/train/audio/audio/\"\n",
    "# Function to create dataset of audio paths and labels\n",
    "def create_dataset(audio_paths, labels):\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(audio_paths)\n",
    "    audio_ds = path_ds.map(lambda x: decode_file(x), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    label_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    return tf.data.Dataset.zip((audio_ds, label_ds))\n",
    "\n",
    "# Function to read and decode audio file\n",
    "def decode_file(path):\n",
    "    audio = tf.io.read_file(path)\n",
    "    audio, _ = tf.audio.decode_wav(audio, 1, sample_rate)\n",
    "    return audio\n",
    "\n",
    "# Function to add noise to audio\n",
    "def add_noise(audio, noises=None, scale=0.5):\n",
    "    if noises is not None:\n",
    "        tf_rnd = tf.random.uniform((tf.shape(audio)[0],), 0, noises.shape[0], dtype=tf.int32)\n",
    "        noise = tf.gather(noises, tf_rnd, axis=0)\n",
    "        prop = tf.math.reduce_max(audio, axis=1) / tf.math.reduce_max(noise, axis=1)\n",
    "        prop = tf.repeat(tf.expand_dims(prop, axis=1), tf.shape(audio)[1], axis=1)\n",
    "        audio = audio + noise * prop * scale\n",
    "    return audio\n",
    "\n",
    "# Function to convert audio wave to FFT\n",
    "def fft(audio):\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    fft = tf.signal.fft(tf.cast(tf.complex(real=audio, imag=tf.zeros_like(audio)), tf.complex64))\n",
    "    fft = tf.expand_dims(fft, axis=-1)\n",
    "    return tf.math.abs(fft[:, : (audio.shape[1] // 2), :])\n",
    "\n",
    "# Get audio file paths and corresponding labels\n",
    "class_names = os.listdir(root)\n",
    "audio_paths = []\n",
    "labels = []\n",
    "for label, name in enumerate(class_names):\n",
    "    if name == '.DS_Store':\n",
    "        continue\n",
    "    dir_path = Path(root) / name\n",
    "    if not dir_path.is_dir():\n",
    "        continue\n",
    "    speaker_sample_paths = [\n",
    "        os.path.join(dir_path, filepath)\n",
    "        for filepath in os.listdir(dir_path)\n",
    "        if filepath.endswith(\".wav\")\n",
    "    ]\n",
    "    audio_paths += speaker_sample_paths\n",
    "    labels += [label] * len(speaker_sample_paths)\n",
    "\n",
    "\n",
    "# Define validation split percentage\n",
    "validation = 0.1\n",
    "shuffle = 43\n",
    "sample_rate = 16000\n",
    "# Shuffle the data\n",
    "rng = np.random.RandomState(shuffle)\n",
    "rng.shuffle(audio_paths)\n",
    "rng = np.random.RandomState(shuffle)\n",
    "rng.shuffle(labels)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "num_val_samples = int(validation * len(audio_paths))\n",
    "train_audio_paths = audio_paths[:-num_val_samples]\n",
    "train_labels = labels[:-num_val_samples]\n",
    "valid_audio_paths = audio_paths[-num_val_samples:]\n",
    "valid_labels = labels[-num_val_samples:]\n",
    "\n",
    "\n",
    "\n",
    "scale = 0.5 # for noise addition\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "\n",
    "# Create datasets for training and validation\n",
    "train_ds = create_dataset(train_audio_paths, train_labels)\n",
    "train_ds = train_ds.shuffle(buffer_size=batch_size * 8, seed=shuffle).batch(batch_size)\n",
    "valid_ds = create_dataset(valid_audio_paths, valid_labels)\n",
    "valid_ds = valid_ds.shuffle(buffer_size=32 * 8, seed=shuffle).batch(32)\n",
    "\n",
    "# Apply transformations to datasets\n",
    "train_ds = train_ds.map(lambda x, y: (fft(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "valid_ds = valid_ds.map(lambda x, y: (fft(x), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "valid_ds = valid_ds.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Define residual block for the model\n",
    "def residual_block(x, filters, conv_num=3, activation=\"relu\", dropout_rate=0.2):\n",
    "    s = keras.layers.Conv1D(filters, 1, padding=\"same\")(x)\n",
    "    for i in range(conv_num - 1):\n",
    "        x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n",
    "        x = keras.layers.Activation(activation)(x)\n",
    "        x = keras.layers.Dropout(dropout_rate)(x)\n",
    "    x = keras.layers.Conv1D(filters, 3, padding=\"same\")(x)\n",
    "    x = keras.layers.Add()([x, s])\n",
    "    x = keras.layers.Activation(activation)(x)\n",
    "    return keras.layers.MaxPool1D(pool_size=2, strides=2)(x)\n",
    "\n",
    "# Build the model\n",
    "def build_model(input_shape, num_classes, dropout_rate=0.2):\n",
    "    inputs = keras.layers.Input(shape=input_shape, name=\"input\")\n",
    "    x = residual_block(inputs, 16, 2, dropout_rate=dropout_rate)\n",
    "    x = residual_block(x, 32, 2, dropout_rate=dropout_rate)\n",
    "    x = residual_block(x, 64, 3, dropout_rate=dropout_rate)\n",
    "    x = residual_block(x, 128, 3, dropout_rate=dropout_rate)\n",
    "    x = residual_block(x, 128, 3, dropout_rate=dropout_rate)\n",
    "    x = keras.layers.AveragePooling1D(pool_size=3, strides=3)(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = keras.layers.Dropout(dropout_rate)(x)\n",
    "    x = keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "    outputs = keras.layers.Dense(num_classes, activation=\"softmax\", name=\"output\")(x)\n",
    "    return keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile the model\n",
    "dropout_rate = 0.2\n",
    "model = build_model((sample_rate // 2, 1), len(class_names), dropout_rate=dropout_rate)\n",
    "model.compile(optimizer=\"Adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Define callbacks\n",
    "earlystopping_cb = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "mdlcheckpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "    \"speaker_recognition_model.h5\", monitor=\"val_accuracy\", save_best_only=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_ds,\n",
    "    callbacks=[earlystopping_cb, mdlcheckpoint_cb],\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "print(model.evaluate(valid_ds))\n",
    "\n",
    "# Save class names to JSON file\n",
    "import json\n",
    "with open(\"class_names.json\", \"w\") as f:\n",
    "    json.dump(class_names, f)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"model2.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted Speaker: irani\n"
     ]
    }
   ],
   "source": [
    "import wave\n",
    "import numpy as np\n",
    "from scipy.signal import resample\n",
    "\n",
    "# Open the WAV file\n",
    "wav_file = wave.open('../sample/irani/4.wav', 'r')\n",
    "\n",
    "frames = wav_file.getnframes()\n",
    "frame_rate = wav_file.getframerate()\n",
    "channels = wav_file.getnchannels()\n",
    "sample_width = wav_file.getsampwidth()\n",
    "\n",
    "# Read the frames from the WAV file\n",
    "frames = wav_file.readframes(frames)\n",
    "\n",
    "# Convert the frames to a NumPy array\n",
    "frames = np.frombuffer(frames, dtype=np.int16)\n",
    "\n",
    "# Resample the audio to match the model's input sampling rate and length\n",
    "target_length = sample_rate // 2  # Model expects input of length 8000\n",
    "resampled_frames = resample(frames, target_length)\n",
    "\n",
    "# Expand the dimensions to match the model's input shape\n",
    "audio_input = np.expand_dims(resampled_frames, axis=-1)\n",
    "\n",
    "wav_file.close()\n",
    "\n",
    "prediction = model.predict(np.expand_dims(audio_input, axis=0))\n",
    "predicted_label_index = np.argmax(prediction)\n",
    "predicted_speaker = class_names[predicted_label_index]\n",
    "print(\"Predicted Speaker:\", predicted_speaker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
